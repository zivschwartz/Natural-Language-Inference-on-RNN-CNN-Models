# Natural-Language-Inference-on-RNN-CNN-Models
Natural Language Inference on RNN and CNN Models for Natural Language Processing (DS-GA 1011)

Construct an optimal Recurrent or Convolutional Neural Network (RNN/CNN) model that can take on the Stanford Natural Language Inference (SNLI) task. The SNLI data contains two pieces of text: a premise and a hypothesis. After training a model, predictions will be able to be made labeling the premise as **entailing** the hypothesis, **contradicting** the hypothesis, or neither *entailing* nor *contradicting* and thus being **neutral** to it. As this task is based on natural text, ambiguity will be introduced regarding the labeling of each premise and hypothesis pair. The SNLI dataset contains 100,000 training instances and the 1,000 validation instances. After the best model is built, testing on the Multi-Genre Natural Language Inference (MNLI) -- a variant of SNLI which covers spoken and written text from different sources, or “genres” -- is performed to determine how well the optimal model performs on different genres. 
